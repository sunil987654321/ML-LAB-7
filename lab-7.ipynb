{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding AND gate using suitable hyperparameters and applying RandomSearchCV()\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Best hyperparameters: {'C': 0.7337391594646552, 'penalty': 'l2'}\n",
      "Best score: 0.9030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SUNIL KUMAR REDDY\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "215 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "215 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\SUNIL KUMAR REDDY\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\SUNIL KUMAR REDDY\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SUNIL KUMAR REDDY\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SUNIL KUMAR REDDY\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\SUNIL KUMAR REDDY\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [  nan 0.903   nan   nan 0.903 0.903 0.903 0.903 0.903 0.896 0.903   nan\n",
      "   nan   nan 0.903   nan   nan 0.903 0.903   nan   nan 0.903 0.903 0.903\n",
      "   nan 0.903   nan 0.903 0.901 0.903 0.903 0.903 0.903 0.901 0.903 0.903\n",
      " 0.903 0.903   nan   nan 0.903 0.903 0.903   nan   nan   nan 0.903 0.903\n",
      "   nan 0.903   nan 0.903   nan   nan 0.903   nan 0.903 0.903   nan   nan\n",
      " 0.903   nan   nan   nan   nan 0.903   nan   nan 0.903   nan   nan   nan\n",
      " 0.903   nan 0.903 0.903 0.903 0.903 0.903 0.903 0.903 0.903   nan   nan\n",
      "   nan 0.903   nan   nan 0.903   nan 0.903 0.903   nan 0.903 0.903   nan\n",
      "   nan 0.903 0.903 0.901]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Applying RandomSearchCV() for a single perceptron AND gate\n",
    "print(\"Finding AND gate using suitable hyperparameters and applying RandomSearchCV()\")\n",
    "\n",
    "# Generate a random classification dataset for AND gate\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_clusters_per_class=1,\n",
    "                           n_redundant=0, random_state=42)\n",
    "\n",
    "# Define the model and hyperparameter distributions\n",
    "model = LogisticRegression(random_state=42)\n",
    "param_dist = {'C': uniform(loc=0, scale=4),\n",
    "              'penalty': ['l1', 'l2']}\n",
    "\n",
    "# Create the RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist,\n",
    "                                   n_iter=100, cv=5, random_state=42,\n",
    "                                   n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the RandomizedSearchCV object to the data\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and score\n",
    "print(f\"Best hyperparameters: {random_search.best_params_}\")\n",
    "print(f\"Best score: {random_search.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier                     Best Hyperparameters                               Best Score     \n",
      "====================================================================================================\n",
      "LogisticRegression             {'C': 0.08233797718320979, 'penalty': 'l2'}        0.9600         \n",
      "SVC                            {'C': 0.8493564427131046, 'gamma': 0.18182496720710062, 'kernel': 'rbf'} 0.9600         \n",
      "DecisionTreeClassifier         {'max_depth': 3, 'min_samples_leaf': 6, 'min_samples_split': 6} 0.9400         \n",
      "RandomForestClassifier         {'max_depth': 7, 'min_samples_split': 5, 'n_estimators': 142} 0.9400         \n",
      "CatBoostClassifier             {'depth': 4, 'iterations': 98, 'learning_rate': 0.2723873301291946} 0.9500         \n",
      "AdaBoostClassifier             {'learning_rate': 2.329163764267956, 'n_estimators': 124} 0.9400         \n",
      "XGBClassifier                  {'learning_rate': 0.19727005942368125, 'max_depth': 7, 'n_estimators': 64} 0.9400         \n",
      "GaussianNB                     {'var_smoothing': 3.746401188473625e-08}           0.9200         \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Generate a random classification dataset\n",
    "X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n",
    "\n",
    "# Storing all the necessary classifiers that are required\n",
    "classifiers = [\n",
    "    LogisticRegression(random_state=42),\n",
    "    SVC(probability=True),\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    CatBoostClassifier(random_state=42, verbose=0),\n",
    "    AdaBoostClassifier(random_state=42),\n",
    "    XGBClassifier(random_state=42, use_label_encoder=False),\n",
    "    GaussianNB()\n",
    "]\n",
    "\n",
    "# Setting the parameter descriptions \n",
    "param_dists = [\n",
    "    {'C': uniform(loc=0, scale=4), 'penalty': ['l2']},  # Only 'l2' penalty for Logistic Regression\n",
    "    {'C': uniform(loc=0, scale=4), 'gamma': uniform(loc=0, scale=1), 'kernel': ['rbf', 'poly', 'sigmoid']},\n",
    "    {'max_depth': randint(low=1, high=20), 'min_samples_leaf': randint(low=1, high=10), 'min_samples_split': randint(low=2, high=10)},\n",
    "    {'max_depth': randint(low=1, high=20), 'n_estimators': randint(low=50, high=200), 'min_samples_split': randint(low=2, high=10)},\n",
    "    {'depth': randint(low=4, high=10), 'iterations': randint(low=50, high=200), 'learning_rate': uniform(loc=0.01, scale=0.5)},\n",
    "    {'n_estimators': randint(low=50, high=200), 'learning_rate': uniform(loc=0.1, scale=5)},\n",
    "    {'max_depth': randint(low=3, high=10), 'n_estimators': randint(low=50, high=200), 'learning_rate': uniform(loc=0.01, scale=0.5)},\n",
    "    {'var_smoothing': uniform(loc=1e-11, scale=1e-7)}\n",
    "]\n",
    "\n",
    "print(\"{:<30} {:<50} {:<15}\".format('Classifier', 'Best Hyperparameters', 'Best Score'))\n",
    "print(\"=\"*100)\n",
    "\n",
    "for model, param_dist in zip(classifiers, param_dists):\n",
    "    random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=25, cv=5, random_state=42, n_jobs=-1, verbose=0)\n",
    "    random_search.fit(X, y)\n",
    "    print(\"{:<30} {:<50} {:<15}\".format(model.__class__.__name__, str(random_search.best_params_), \"{:.4f}\".format(random_search.best_score_)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
